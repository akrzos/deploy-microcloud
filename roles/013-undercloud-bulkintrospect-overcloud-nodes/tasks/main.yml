---
#
# Perform Bulk Introspection of Overcloud Nodes
#
# Basic flow of this role:
# * Introspect
# * Clean up clean wait/failed
# * Check if any other failed to introspect nodes and cleanup (1024 memory)
# * Dump Node Data
#

- name: Bulk Introspection Block
  block:
    - name: Run Bulk introspection
      shell:  |
        set -o pipefail
        . /home/stack/stackrc
        { time timeout {{bulk_introspection_timeout}} openstack overcloud node introspect --all-manageable --provide 2>&1 | tee -a {{log_dir}}/013-0-bulk-introspection.log ; } 2>> {{log_dir}}/013-0-bulk-introspection.log
      register: bulk_introspection_result
      ignore_errors: true

    # If introspection had a failure it may not move nodes into available
    - name: Set Nodes to provide
      shell: |
        set -o pipefail
        . /home/stack/stackrc
        { time timeout {{node_provide_timeout}} openstack overcloud node provide --all-manageable 2>&1 | tee -a {{log_dir}}/013-1-overcloud-node-provide.log ; } 2>> {{log_dir}}/013-1-overcloud-node-provide.log
      when: bulk_introspection_result.failed

    - name: Check for stuck/failed nodes in clean failed/clean wait
      shell: |
        . /home/stack/stackrc
        openstack baremetal node list | egrep "clean failed|clean wait" -i -c
      register: nodes_stuck
      failed_when: nodes_stuck.rc == 0
  rescue:
     # Collect additional items to sweep into mccloud-log for debugging in event of failure
    - name: Collect OpenStack Ironic log files
      become: true
      shell: |
        mkdir -p {{log_dir}}/ironic
        cp -r /var/log/ironic/* {{log_dir}}/ironic

    - name: Dump baremetal node list
      shell:  |
        set -o pipefail
        . /home/stack/stackrc
        openstack baremetal node list 2>&1 | tee -a {{log_dir}}/013-2-node-list.log
        openstack baremetal node list | grep None | awk '{print $2}' | xargs -I % openstack baremetal node show % 2>&1 | tee -a {{log_dir}}/013-2-node-show.log

    - name: Check if failed nodes tolerated
      fail:
        msg: "Failed nodes and node_failure_tolerance is false"
      when: not node_failure_tolerance

    - name: Count failed nodes (nodes stuck in clean failed/wait after timeout)
      shell: |
        . /home/stack/stackrc
        openstack baremetal node list | egrep "clean failed|clean wait" -i -c
      ignore_errors: true
      register: failed_node_count

    - name: Accumlate node failures into total_node_failure_count
      set_fact:
        total_node_failure_count: "{{total_node_failure_count|int + failed_node_count.stdout|int}}"

    # If we tolerate node failures, then check if we failed too many
    - name: "Check failed node count (Failure Count: {{ total_node_failure_count|int}} > Tolerance: {{node_failure_tolerance_count}})"
      fail:
        msg: "Too many failed nodes - {{total_node_failure_count|int}}"
      when: total_node_failure_count|int > node_failure_tolerance_count

    # If node_failure_tolerance_count cleared, we need to remove the "failed nodes" from instackenv.valid.json and OpenStack Ironic
    - name: Remove failed bulk introspection nodes
      shell: |
        set -o pipefail
        . /home/stack/stackrc
        openstack baremetal node list | egrep "clean failed|clean wait" -i | awk '{print $2}' | xargs -I % openstack baremetal node show -c uuid -c driver_info % > {{log_dir}}/013-3-nodes-to-delete.log
        for node_ipmi in `openstack baremetal node list | egrep "clean failed|clean wait" -i | awk '{print $2}' | xargs -I % openstack baremetal node show -c driver_info -f yaml % | grep "ipmi_address" | awk '{print $2}'`
        do
          cat /home/stack/instackenv.valid.json | jq  "{ nodes: [ .nodes[] | select(.pm_addr==\"${node_ipmi}\" | not) | . ]}" > /home/stack/intermediate.json
          python -m json.tool /home/stack/intermediate.json /home/stack/instackenv.valid.json
          rm -f /home/stack/intermediate.json
        done
        openstack baremetal node list | egrep "clean wait" -i | awk '{print $2}' | xargs -I % openstack baremetal node abort % 2>&1 | tee -a {{log_dir}}/013-3-node-abort.log
        openstack baremetal node list | egrep "clean failed" -i | awk '{print $2}' | xargs -I % openstack baremetal node maintenance set % 2>&1 | tee -a {{log_dir}}/013-3-node-maintenance-set.log
        openstack baremetal node list | egrep "clean failed" -i | awk '{print $2}' | xargs -I % openstack baremetal node delete % 2>&1 | tee -a {{log_dir}}/013-3-node-delete-clean-failed.log
  always:
    - name: Recollect instackenv.valid.json
      shell: |
        cp /home/stack/instackenv.valid.json {{log_dir}}/instackenv.valid1.clean.json
      ignore_errors: true

    - name: Collect Bulk Introspection Artifacts
      synchronize:
        src: "{{log_dir}}"
        dest: "{{artifact_dir}}/mccloud-log"
        mode: pull
      when: collect_artifacts

- name: Check after cleaning up clean wait/failed nodes if other nodes failed to introspect
  block:
    - name: Check for nodes with 1024 memory_mb which indicates failed introspection
      shell: |
        . /home/stack/stackrc
        openstack baremetal node list | grep "None" | awk '{print $2}' | xargs -I % openstack baremetal node show -c properties % | grep "1024" -c
      register: introspection_failures
      failed_when: introspection_failures.rc == 0
  rescue:
     # Collect additional items to sweep into mccloud-log for debugging in event of failure
    - name: Collect OpenStack Ironic log files
      become: true
      shell: |
        mkdir -p {{log_dir}}/ironic
        cp -r /var/log/ironic/* {{log_dir}}/ironic

    - name: Dump baremetal node list
      shell:  |
        set -o pipefail
        . /home/stack/stackrc
        openstack baremetal node list 2>&1 | tee -a {{log_dir}}/013-4-node-list.log
        openstack baremetal node list | grep None | awk '{print $2}' | xargs -I % openstack baremetal node show % 2>&1 | tee -a {{log_dir}}/013-4-node-show.log

    - name: Check if failed nodes tolerated
      fail:
        msg: "Failed nodes and node_failure_tolerance is false"
      when: not node_failure_tolerance

    - name: Count failed nodes (nodes with 1024 memory_mb)
      shell: |
        . /home/stack/stackrc
        openstack baremetal node list | grep "None" | awk '{print $2}' | xargs -I % openstack baremetal node show -c properties % | grep "1024" -c
      ignore_errors: true
      register: failed_node_count

    - name: Accumlate node failures into total_node_failure_count
      set_fact:
        total_node_failure_count: "{{total_node_failure_count|int + failed_node_count.stdout|int}}"

    # If we tolerate node failures, then check if we failed too many
    - name: "Check failed node count (Failure Count: {{ total_node_failure_count|int}} > Tolerance: {{node_failure_tolerance_count}})"
      fail:
        msg: "Too many failed nodes - {{total_node_failure_count|int}}"
      when: total_node_failure_count|int > node_failure_tolerance_count

    - name: Remove failed bulk introspection nodes
      shell: |
        set -o pipefail
        . /home/stack/stackrc
        openstack baremetal node list | grep "None" | awk '{print $2}' | xargs -I % openstack baremetal node show -c driver_info -c uuid -c properties % | grep "1024" -B 1 -A 1 > {{log_dir}}/013-5-nodes-to-delete.log
        for node_ipmi in `cat {{log_dir}}/013-5-nodes-to-delete.log | grep "uuid" | awk '{print $4}' | xargs -I % openstack baremetal node show -c driver_info -f yaml % | grep "ipmi_address" | awk '{print $2}'`
        do
          cat /home/stack/instackenv.valid.json | jq  "{ nodes: [ .nodes[] | select(.pm_addr==\"${node_ipmi}\" | not) | . ]}" > /home/stack/intermediate.json
          python -m json.tool /home/stack/intermediate.json /home/stack/instackenv.valid.json
          rm -f /home/stack/intermediate.json
        done
        cat {{log_dir}}/013-5-nodes-to-delete.log | grep "uuid" | awk '{print $4}' | xargs -I % openstack baremetal node abort % 2>&1 | tee -a {{log_dir}}/013-5-node-abort.log
        cat {{log_dir}}/013-5-nodes-to-delete.log | grep "uuid" | awk '{print $4}' | xargs -I % openstack baremetal node maintenance set % 2>&1 | tee -a {{log_dir}}/013-5-node-maintenance-set.log
        cat {{log_dir}}/013-5-nodes-to-delete.log | grep "uuid" | awk '{print $4}' | xargs -I % openstack baremetal node delete % 2>&1 | tee -a {{log_dir}}/013-5-node-delete-clean-failed.log
  always:
    - name: Recollect instackenv.valid.json
      shell: |
        cp /home/stack/instackenv.valid.json {{log_dir}}/instackenv.valid2.postintrospection.json
      ignore_errors: true

    - name: Collect Bulk Introspection Artifacts
      synchronize:
        src: "{{log_dir}}"
        dest: "{{artifact_dir}}/mccloud-log"
        mode: pull
      when: collect_artifacts

- name: Introspection-data Block
  block:
    - name: Generate introspection-data
      shell: |
        . /home/stack/stackrc
        mkdir /home/stack/introspection-data
        for i in `openstack baremetal node list | grep 'None' | awk '{print $2}'`; do openstack baremetal introspection data save $i > /home/stack/introspection-data/$i; done
      ignore_errors: true
  always:
    - name: Copy introspection data
      become: true
      shell: |
        mkdir -p {{log_dir}}/introspection-data
        cp -r /home/stack/introspection-data/* {{log_dir}}/introspection-data
      ignore_errors: true

    - name: Collect Introspection-data Artifacts
      synchronize:
        src: "{{log_dir}}"
        dest: "{{artifact_dir}}/mccloud-log"
        mode: pull
      when: collect_artifacts
